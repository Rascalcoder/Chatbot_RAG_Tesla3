---
config:
  layout: dagre
---
flowchart TB
 subgraph UserInterface["üì± STREAMLIT UI"]
        UI1["Chatbot Oldal"]
        UI2["Monitoring Oldal"]
        UI3["Evaluation Oldal"]
  end
 subgraph DocumentUpload["üìÑ DOKUMENTUM FELDOLGOZ√ÅS"]
        DOC1["PDF/DOCX/TXT Upload"]
        DOC2["Sz√∂veg Kinyer√©s<br>+ Oldal Marker"]
        DOC3["Chunking<br>1000 char, 200 overlap"]
        DOC4["Embedding<br>all-MiniLM-L6-v2"]
        DOC5[("ChromaDB Persist<br>480 chunks")]
  end
 subgraph QueryPipeline["üîÑ QUERY PIPELINE"]
        Q1["User Query<br>HU/EN"]
        Q2{"Magyar?"}
        Q3["OpenAI Translation<br>+ Cache"]
        Q4["Query Embedding"]
        Q5["Vector Search<br>Top-10"]
        Q6["Dynamic Filter<br>threshold + min 2"]
        Q7["Cross-Encoder<br>Reranking"]
        Q8["Context Format<br>+ Metadata"]
        Q9["LLM Generation<br>GPT-3.5"]
        Q10["Response + Sources"]
  end
 subgraph Monitoring["üìä MONITORING & METRICS"]
        M1["Metrics Collector"]
        M2["LLM Call Counter"]
        M3["Token & Cost Tracker"]
        M4["Latency Recorder"]
        M5["User Feedback<br>üëçüëé"]
        M6[("SQLite/JSON<br>Metrics DB")]
  end
 subgraph RAGEval["1Ô∏è‚É£ RAG-LEVEL EVAL"]
        R1["Retrieval Quality"]
        R2["Precision/Recall/MRR"]
        R3["Embedding Performance"]
        R4["Chunking Efficiency"]
        R5["20+ Test Cases"]
  end
 subgraph PromptEval["2Ô∏è‚É£ PROMPT-LEVEL EVAL"]
        P1["Context Relevance"]
        P2["Hallucination Detection"]
        P3["LLM-as-Judge"]
        P4["Response Quality"]
        P5["5+ Test Cases"]
  end
 subgraph AppEval["3Ô∏è‚É£ APP-LEVEL EVAL"]
        A1["User Journey Tests"]
        A2["E2E Latency"]
        A3["System Performance"]
        A4["Error Handling"]
        A5["3+ Scenarios"]
  end
 subgraph Evaluation["üß™ EVALUATION FRAMEWORK"]
    direction TB
        RAGEval
        PromptEval
        AppEval
        EVAL_RESULTS["üìà Evaluation Results<br>Reports + Timestamps"]
  end
 subgraph Feedback["üîÅ FEEDBACK LOOP"]
        F1{"Eval Results<br>OK?"}
        F2["Threshold Tuning"]
        F3["Prompt Refinement"]
        F4["Reranking Optimization"]
        F5["‚úÖ System Validated"]
  end
    DOC1 --> DOC2
    DOC2 --> DOC3
    DOC3 --> DOC4
    DOC4 --> DOC5
    UI1 --> Q1
    Q1 --> Q2
    Q2 -- Yes --> Q3
    Q2 -- No --> Q4
    Q3 --> Q4
    Q4 --> Q5
    DOC5 --> Q5
    Q5 --> Q6
    Q6 --> Q7
    Q7 --> Q8
    Q8 --> Q9
    Q9 --> Q10
    Q10 --> UI1
    Q3 -.-> M1
    Q5 -.-> M1
    Q7 -.-> M1
    Q9 -.-> M1
    M1 --> M2 & M3 & M4
    Q10 -.-> M5
    M2 --> M6
    M3 --> M6
    M4 --> M6
    M5 --> M6
    M6 --> UI2
    UI3 --> RAGEval & PromptEval & AppEval
    RAGEval --> R1
    R1 --> R2 & R3 & R4
    R2 --> R5
    R3 --> R5
    R4 --> R5
    PromptEval --> P1
    P1 --> P2
    P2 --> P3
    P3 --> P4
    P4 --> P5
    AppEval --> A1
    A1 --> A2
    A2 --> A3
    A3 --> A4
    A4 --> A5
    R5 --> EVAL_RESULTS
    P5 --> EVAL_RESULTS
    A5 --> EVAL_RESULTS
    EVAL_RESULTS --> UI3 & F1
    F1 -- Issues --> F2 & F3 & F4
    F2 --> DOC5
    F3 --> Q9
    F4 --> Q7
    F1 -- Pass --> F5
    RAGEval -. Test Queries .-> Q5
    PromptEval -. Test Prompts .-> Q9
    AppEval -. E2E Tests .-> Q1

    style UI1 fill:#4CAF50,stroke:#2E7D32,color:#fff
    style UI2 fill:#2196F3,stroke:#1565C0,color:#fff
    style UI3 fill:#FF9800,stroke:#E65100,color:#fff
    style DOC5 fill:#9C27B0,stroke:#4A148C,color:#fff
    style M6 fill:#00BCD4,stroke:#006064,color:#fff
    style EVAL_RESULTS fill:#FFC107,stroke:#F57F17,color:#000
    style F5 fill:#4CAF50,stroke:#2E7D32,color:#fff
