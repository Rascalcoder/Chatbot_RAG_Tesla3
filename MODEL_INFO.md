# Modell Inform√°ci√≥k

## üéØ T√°mogatott Konfigur√°ci√≥k

A projekt **HIBRID konfigur√°ci√≥t** t√°mogat - v√°laszthatsz a rendszered szerint.

### Konfigur√°ci√≥ A: Hibrid (Aj√°nlott 8 GB RAM-hoz)
**Embedding**: Helyi kis modell
**LLM**: OpenAI felh≈ë API

### Konfigur√°ci√≥ B: Teljes Lok√°lis (16+ GB RAM-hoz)
**Embedding**: Helyi nagy modell
**LLM**: Helyi Qwen modell

---

## Embedding Modellek

### üîπ MiniLM (Kis, Gyors)
- **Modell**: `sentence-transformers/all-MiniLM-L6-v2`
- **Dimenzi√≥**: 384
- **M√©ret**: ~90 MB
- **RAM ig√©ny**: ~500 MB
- **Haszn√°lat**: Kisebb rendszerekhez, HIBRID konfigur√°ci√≥hoz
- **K√∂nyvt√°r**: sentence-transformers

**El≈ëny√∂k**: Gyors, kis mem√≥riaig√©ny, megfelel≈ë pontoss√°g
**H√°tr√°nyok**: Kisebb dimenzi√≥ (384 vs 1024)

### üîπ BGE-M3 (Nagy, Pontos)
- **Modell**: `BAAI/bge-m3`
- **Dimenzi√≥**: 1024
- **M√©ret**: ~1.2 GB
- **RAM ig√©ny**: ~2-3 GB
- **Haszn√°lat**: Teljes lok√°lis konfigur√°ci√≥hoz
- **K√∂nyvt√°r**: FlagEmbedding vagy sentence-transformers

**El≈ëny√∂k**: Magas pontoss√°g, multilingual t√°mogat√°s
**H√°tr√°nyok**: Nagyobb mem√≥riaig√©ny

**Telep√≠t√©s**:
```bash
pip install FlagEmbedding sentence-transformers
```

---

## LLM Modellek

### üîπ OpenAI GPT-3.5-turbo (Felh≈ë)
- **Modell**: `gpt-3.5-turbo`
- **T√≠pus**: OpenAI API
- **K√∂lts√©g**: ~$0.0005-0.001 / v√°lasz
- **RAM ig√©ny**: 0 GB (felh≈ë)
- **Sebess√©g**: Gyors (~1-3 m√°sodperc)
- **Haszn√°lat**: HIBRID konfigur√°ci√≥hoz

**El≈ëny√∂k**: Nincs RAM ig√©ny, gyors, megb√≠zhat√≥
**H√°tr√°nyok**: API kulcs √©s internet sz√ºks√©ges, k√∂lts√©g

**Be√°ll√≠t√°s**:
```env
OPENAI_API_KEY=your_api_key_here
LLM_MODEL=gpt-3.5-turbo
```

### üîπ Qwen3-4B (Lok√°lis)
- **Modell**: `Qwen/Qwen3-4B-Instruct-2507`
- **T√≠pus**: Instruction-tuned nyelvi modell
- **M√©ret**: ~8-10 GB
- **RAM ig√©ny**: ~10-12 GB
- **Sebess√©g**: Lassabb CPU-n (~10-30s), gyors GPU-n (~1-3s)
- **Haszn√°lat**: Teljes lok√°lis konfigur√°ci√≥hoz
- **K√∂nyvt√°r**: transformers (Hugging Face)
- **T√°mogat√°s**: Streaming v√°laszok

**El≈ëny√∂k**: Ingyenes, priv√°t, offline m≈±k√∂d√©s
**H√°tr√°nyok**: Nagy RAM ig√©ny, lassabb CPU-n

**Telep√≠t√©s**:
```bash
pip install transformers torch accelerate
```

**Els≈ë haszn√°latkor** automatikusan let√∂lt≈ëdik a Hugging Face-r≈ël.

## Hardver K√∂vetelm√©nyek

### Minim√°lis
- **RAM**: 16 GB (8 GB modell + 8 GB rendszer)
- **GPU**: Opcion√°lis, de aj√°nlott (CUDA kompatibilis)
- **T√°rhely**: ~15 GB (modellek + adatok)

### Aj√°nlott
- **RAM**: 32 GB
- **GPU**: NVIDIA GPU 8+ GB VRAM (pl. RTX 3060, RTX 4060)
- **T√°rhely**: 20+ GB

## GPU Haszn√°lat

Ha NVIDIA GPU-d van, a modellek automatikusan GPU-ra t√∂lt≈ëdnek:
- CUDA 11.8+ sz√ºks√©ges
- PyTorch automatikusan detekt√°lja a GPU-t

CPU-n is m≈±k√∂dik, de lassabb lesz.

## Alternat√≠v Modellek

Ha m√°s modelleket szeretn√©l haszn√°lni, m√≥dos√≠tsd a `.env` f√°jlban:

```env
EMBEDDING_MODEL=egy√©b-embedding-modell
LLM_MODEL=egy√©b-llm-modell
```

Vagy a k√≥dban k√∂zvetlen√ºl:
```python
embedding_model = EmbeddingModel(model_name="egy√©b-modell", use_openai=False)
llm_generator = LLMGenerator(model_name="egy√©b-modell", use_openai=False)
```

## Teljes√≠tm√©ny Optimaliz√°l√°s

### Quantiz√°ci√≥ (8-bit)
A Qwen modell 8-bit quantiz√°ci√≥val is haszn√°lhat√≥ kevesebb mem√≥ri√°hoz:

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0
)
```

### Batch Processing
A BGE-M3 embedding batch-ben is futtathat√≥ t√∂bb dokumentumhoz egyszerre.

## Hibaelh√°r√≠t√°s

### "Out of Memory" hiba
- Cs√∂kkentsd a `max_tokens` √©rt√©k√©t
- Haszn√°lj 8-bit quantiz√°ci√≥t
- Cs√∂kkentsd a batch m√©retet

### Lass√∫ gener√°l√°s
- Haszn√°lj GPU-t ha van
- Cs√∂kkentsd a `max_tokens` √©rt√©k√©t
- Haszn√°lj streaming-et a jobb UX-hez

### Modell let√∂lt√©si hiba
- Ellen≈ërizd az internetkapcsolatot
- Pr√≥b√°ld meg manu√°lisan let√∂lteni a Hugging Face-r≈ël
- Haszn√°lj Hugging Face token-t ha sz√ºks√©ges

