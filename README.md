# RAG AlapÃº AI Asszisztens

Ez a projekt egy teljes kÃ¶rÅ± RAG (Retrieval-Augmented Generation) alapÃº AI asszisztens implementÃ¡ciÃ³ja, amely kÃ©pes dokumentumok feldolgozÃ¡sÃ¡ra, relevÃ¡ns informÃ¡ciÃ³k visszakeresÃ©sÃ©re Ã©s intelligens vÃ¡laszok generÃ¡lÃ¡sÃ¡ra.

## ğŸš€ FunkciÃ³k

### RAG Rendszer ArchitektÃºra
- âœ… Dokumentum feldolgozÃ¡s Ã©s chunking stratÃ©gia
- âœ… Embedding modell integrÃ¡ciÃ³ (OpenAI, lokÃ¡lis alternatÃ­vÃ¡k)
- âœ… Vektor adatbÃ¡zis (ChromaDB)
- âœ… Retrieval Ã©s reranking mechanizmus
- âœ… LLM integrÃ¡ciÃ³ vÃ¡laszgenerÃ¡lÃ¡shoz (streaming tÃ¡mogatÃ¡ssal)

### AlkalmazÃ¡s FunkciÃ³k
- âœ… Webes felÃ¼let (Streamlit)
- âœ… Dokumentum feltÃ¶ltÃ©s Ã©s kezelÃ©s
- âœ… Streaming vÃ¡laszok tÃ¡mogatÃ¡sa
- âœ… Session/conversation management

### HÃ¡romszintÅ± Evaluation Framework
- âœ… **RAG szintÅ± Ã©rtÃ©kelÃ©s**: Retrieval minÅ‘sÃ©g (precision, recall, MRR), embedding modell teljesÃ­tmÃ©ny, chunking stratÃ©gia hatÃ©konysÃ¡ga
- âœ… **Prompt szintÅ± Ã©rtÃ©kelÃ©s**: Single-turn eval, context relevance, hallucinÃ¡ciÃ³ detektÃ¡lÃ¡s, LLM-as-Judge
- âœ… **AlkalmazÃ¡s szintÅ± Ã©rtÃ©kelÃ©s**: Teljes user journey tesztelÃ©s, response quality, latency Ã©s performance metrikÃ¡k

### Monitoring Ã©s Analitika
- âœ… Token hasznÃ¡lat Ã©s kÃ¶ltsÃ©g tracking
- âœ… Latency metrikÃ¡k (first token, total response time)

## ğŸ“‹ TelepÃ­tÃ©s

### ElÅ‘feltÃ©telek
- Python 3.9 vagy Ãºjabb
- pip vagy conda

### LÃ©pÃ©sek

1. **Repository klÃ³nozÃ¡sa**
```bash
git clone <repository-url>
cd "7.het_ZÃ¡rÃ³ projekt"
```

2. **VirtuÃ¡lis kÃ¶rnyezet lÃ©trehozÃ¡sa (ajÃ¡nlott)**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# vagy
venv\Scripts\activate  # Windows
```

3. **FÃ¼ggÅ‘sÃ©gek telepÃ­tÃ©se**
```bash
pip install -r requirements.txt
```

4. **KÃ¶rnyezeti vÃ¡ltozÃ³k beÃ¡llÃ­tÃ¡sa**
```bash
cp .env.example .env
# Szerkeszd a .env fÃ¡jlt Ã©s add hozzÃ¡ az API kulcsokat
```

5. **AlkalmazÃ¡s indÃ­tÃ¡sa**
```bash
streamlit run app.py
```

A bÃ¶ngÃ©szÅ‘ben automatikusan megnyÃ­lik a `http://localhost:8501` cÃ­men.

## ğŸ”§ KonfigurÃ¡ciÃ³

A projekt **HIBRID konfigurÃ¡ciÃ³t** tÃ¡mogat - rugalmasan vÃ¡laszthatsz helyi Ã©s felhÅ‘ alapÃº modellek kÃ¶zÃ¶tt:

### ğŸ¯ AjÃ¡nlott KonfigurÃ¡ciÃ³ (8 GB RAM-hoz):
- **Embedding**: `sentence-transformers/all-MiniLM-L6-v2` (helyi, ~90 MB)
- **LLM**: `gpt-3.5-turbo` (OpenAI API, nincs RAM igÃ©ny)
- **RAM igÃ©ny**: ~1-2 GB âœ…

### ğŸš€ Teljes LokÃ¡lis KonfigurÃ¡ciÃ³ (16+ GB RAM-hoz):
- **Embedding**: `BAAI/bge-m3` (helyi, ~1.2 GB)
- **LLM**: `Qwen/Qwen3-4B-Instruct-2507` (helyi, ~8-10 GB)
- **RAM igÃ©ny**: ~12-15 GB

A `.env` fÃ¡jlban beÃ¡llÃ­thatÃ³ kÃ¶rnyezeti vÃ¡ltozÃ³k:
- `OPENAI_API_KEY`: OpenAI API kulcs (ha OpenAI LLM-et hasznÃ¡lsz)
- `EMBEDDING_MODEL`: Embedding modell neve
  - LokÃ¡lis: `BAAI/bge-m3`, `sentence-transformers/all-MiniLM-L6-v2`
  - OpenAI: `text-embedding-ada-002`
- `LLM_MODEL`: LLM modell neve
  - LokÃ¡lis: `Qwen/Qwen3-4B-Instruct-2507`
  - OpenAI: `gpt-3.5-turbo`, `gpt-4`
- `VECTOR_DB_PATH`: Vektor adatbÃ¡zis elÃ©rÃ©si Ãºtja
- `CHUNK_SIZE`: Dokumentum chunk mÃ©ret (alapÃ©rtelmezett: 1000)
- `CHUNK_OVERLAP`: Chunk Ã¡tfedÃ©s (alapÃ©rtelmezett: 200)

**Fontos**: A helyi modellek automatikusan letÃ¶ltÅ‘dnek az elsÅ‘ hasznÃ¡latkor. A rendszer automatikusan felismeri az OpenAI modelleket (ha a modell nÃ©v tartalmazza a "gpt-" elÅ‘tagot).

## ğŸ“ Projekt StruktÃºra

```
.
â”œâ”€â”€ app.py                      # Streamlit fÅ‘alkalmazÃ¡s
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ rag/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ document_processor.py    # Dokumentum feldolgozÃ¡s
â”‚   â”‚   â”œâ”€â”€ chunking.py              # Chunking stratÃ©gia
â”‚   â”‚   â”œâ”€â”€ embeddings.py            # Embedding kezelÃ©s
â”‚   â”‚   â”œâ”€â”€ vector_store.py           # Vektor adatbÃ¡zis
â”‚   â”‚   â”œâ”€â”€ retrieval.py              # Retrieval mechanizmus
â”‚   â”‚   â””â”€â”€ reranking.py              # Reranking
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py              # LLM vÃ¡laszgenerÃ¡lÃ¡s
â”‚   â”‚   â””â”€â”€ streaming.py              # Streaming tÃ¡mogatÃ¡s
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ rag_eval.py                # RAG szintÅ± Ã©rtÃ©kelÃ©s
â”‚   â”‚   â”œâ”€â”€ prompt_eval.py             # Prompt szintÅ± Ã©rtÃ©kelÃ©s
â”‚   â”‚   â”œâ”€â”€ app_eval.py                # AlkalmazÃ¡s szintÅ± Ã©rtÃ©kelÃ©s
â”‚   â”‚   â””â”€â”€ test_cases.py              # Teszt esetek
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py                 # MetrikÃ¡k gyÅ±jtÃ©se
â”‚   â”‚   â””â”€â”€ analytics.py               # Analitika
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ session_manager.py         # Session kezelÃ©s
â”œâ”€â”€ data/
â”‚   â””â”€â”€ documents/                    # FeltÃ¶ltÃ¶tt dokumentumok
â”œâ”€â”€ evaluations/
â”‚   â”œâ”€â”€ rag_evaluation_results.json
â”‚   â”œâ”€â”€ prompt_evaluation_results.json
â”‚   â””â”€â”€ app_evaluation_results.json
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_rag.py
â”‚   â”œâ”€â”€ test_retrieval.py
â”‚   â””â”€â”€ test_evaluation.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ§ª Evaluation FuttatÃ¡sa

### RAG SzintÅ± Evaluation
```bash
python -m src.evaluation.rag_eval
```

### Prompt SzintÅ± Evaluation
```bash
python -m src.evaluation.prompt_eval
```

### AlkalmazÃ¡s SzintÅ± Evaluation
```bash
python -m src.evaluation.app_eval
```

## ğŸ“Š Monitoring Dashboard

A monitoring dashboard elÃ©rhetÅ‘ a Streamlit alkalmazÃ¡sban a "Monitoring" oldalon, ahol megtekinthetÅ‘k:
- Token hasznÃ¡lat statisztikÃ¡k
- Latency metrikÃ¡k
- KÃ¶ltsÃ©g tracking
- HasznÃ¡lati trendek

## ğŸ“ HasznÃ¡lat

### AlapvetÅ‘ HasznÃ¡lat

1. **Dokumentum feltÃ¶ltÃ©s**: A fÅ‘oldalon tÃ¶lts fel PDF, TXT vagy DOCX fÃ¡jlokat
2. **KÃ©rdÃ©sek feltevÃ©se**: A chat felÃ¼leten tegyÃ©l fel kÃ©rdÃ©seket a feltÃ¶ltÃ¶tt dokumentumokrÃ³l
3. **EredmÃ©nyek megtekintÃ©se**: A vÃ¡laszok streaming formÃ¡ban jelennek meg
4. **Evaluation futtatÃ¡sa**: Az Evaluation oldalon futtathatsz teszteket

### Tesla Model 3 KÃ©zikÃ¶nyv HasznÃ¡lata

Ha a `model_3.pdf` fÃ¡jl a projekt kÃ¶nyvtÃ¡rÃ¡ban van:

**OpciÃ³ 1: Streamlit app-ban**
1. IndÃ­tsd el az app-ot: `streamlit run app.py`
2. A sidebar-ban tÃ¶ltsd fel a `model_3.pdf` fÃ¡jlt
3. KÃ©rdezz a Model 3-rÃ³l!

**OpciÃ³ 2: Teszt script**
```bash
python test_model3_manual.py
```

Ez automatikusan betÃ¶lti a PDF-et Ã©s futtat teszt kÃ©rdÃ©seket, majd interaktÃ­v mÃ³dba lÃ©p.

**OpciÃ³ 3: ElÅ‘re betÃ¶ltÃ©s**
```bash
python load_model3_manual.py
```

Ez elÅ‘re betÃ¶lti a dokumentumot, Ã­gy a Streamlit app indÃ­tÃ¡sakor mÃ¡r elÃ©rhetÅ‘ lesz.

## ğŸ¥ VideÃ³ PrezentÃ¡ciÃ³k

A projekthez 2 db Loom videÃ³ kÃ©szÃ¼l:
1. **Technikai bemutatÃ³** (5 perc): ArchitektÃºra, RAG pipeline, evaluation framework, monitoring
2. **FelhasznÃ¡lÃ³i demo** (5 perc): Dokumentum feltÃ¶ltÃ©s, kÃ©rdÃ©sek, hibakezelÃ©s, teljesÃ­tmÃ©ny

## ğŸ“„ Licenc

Ez a projekt egy zÃ¡rÃ³projekt rÃ©sze.

## ğŸ‘¥ SzerzÅ‘

Projekt kÃ©szÃ­tÅ‘: [NÃ©v]

---

Sikeres hasznÃ¡latot kÃ­vÃ¡nunk! ğŸš€

## ğŸ” HuggingFace token (biztonsÃ¡g)

Ha a projekt HuggingFace modellekhez hozzÃ¡fÃ©rÃ©st igÃ©nyel, a token kezelÃ©se legyen biztonsÃ¡gos:

- AjÃ¡nlott: futtasd a `huggingface-cli login` parancsot (telepÃ­tsd a `huggingface-hub`-ot), ez menti a tokent lokÃ¡lisan a cache-be.
- AlternatÃ­va: hozz lÃ©tre egy lokÃ¡lis `.env` fÃ¡jlt a projekt gyÃ¶kerÃ©ben (ne committeld):

```env
# .env (NE add hozzÃ¡ a git-hez)
HUGGINGFACE_HUB_TOKEN=hf_xxxYOURTOKENxxxxx
```

- Ãœgyelj rÃ¡, hogy a `HUGGINGFACE_HUB_TOKEN.env` vagy `.env` fÃ¡jlok NEM kerÃ¼ljenek a verziÃ³kezelÃ©sbe; a projekt `.gitignore` tartalmazza ezeket.

Ha bizonytalan vagy, hasznÃ¡ld a `huggingface-cli login`-t.

